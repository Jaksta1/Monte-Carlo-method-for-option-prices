{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJtEmFxclebF+XSEZpiRUo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaksta1/Monte-Carlo-method-for-option-prices/blob/main/skrypt%20mgr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YemDJFrqA5Jy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from qmcpy import Sobol\n",
        "\n",
        "# ---------------------------\n",
        "# QMC -> normals (QMCPy Sobol, randomize=True gives full Owen scramble)\n",
        "# ---------------------------\n",
        "def qmc_normals_qmcpy(n_paths, total_dim, seed=None):\n",
        "    \"\"\"Zwraca tablicę (n_paths, total_dim) z punktami Sobola przekształconymi do N(0,1).\"\"\"\n",
        "    sob = Sobol(dimension=total_dim, randomize=True, seed=seed)\n",
        "    u = sob.gen_samples(n_paths)  # shape (n_paths, total_dim)\n",
        "    eps = 1e-16\n",
        "    u = np.clip(u, eps, 1 - eps)\n",
        "    return norm.ppf(u)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Rozszerzony MARS (LDA + GCV)\n",
        "# ---------------------------\n",
        "class ExtendedMARS:\n",
        "    def __init__(self, max_terms=20, min_leaf=10, penalty=3.0):\n",
        "        self.max_terms = int(max_terms)\n",
        "        self.min_leaf = int(min_leaf)\n",
        "        self.penalty = float(penalty)\n",
        "        self.basis = []   # list of (a, knot, sign)\n",
        "        self.coefs = None\n",
        "        self.intercept_ = 0.0\n",
        "\n",
        "    def _project(self, X, a):\n",
        "        return X.dot(a)\n",
        "\n",
        "    def _gcv(self, y, y_pred, n_params):\n",
        "        n = len(y)\n",
        "        rss = np.sum((y - y_pred) ** 2)\n",
        "        C = n_params + self.penalty\n",
        "        if n <= C:\n",
        "            return rss  # fallback\n",
        "        denom = (1.0 - C / n) ** 2\n",
        "        return rss / denom\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y).ravel()\n",
        "        n, d = X.shape\n",
        "        design = np.ones((n, 1))  # intercept\n",
        "        self.basis = []\n",
        "\n",
        "        for step in range(self.max_terms):\n",
        "            coef_curr, *_ = np.linalg.lstsq(design, y, rcond=None)\n",
        "            y_pred_curr = design.dot(coef_curr)\n",
        "            residual = y - y_pred_curr\n",
        "\n",
        "            # candidate directions: LDA on sign of residuals (two classes)\n",
        "            try:\n",
        "                labels = (residual > np.median(residual)).astype(int)\n",
        "                lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "                lda.fit(X, labels)\n",
        "                a_candidate = lda.coef_[0]\n",
        "                if np.linalg.norm(a_candidate) > 0:\n",
        "                    a_candidate = a_candidate / (np.linalg.norm(a_candidate) + 1e-12)\n",
        "                    candidate_dirs = [a_candidate]\n",
        "                else:\n",
        "                    candidate_dirs = [np.eye(d)[j] for j in range(d)]\n",
        "            except Exception:\n",
        "                candidate_dirs = [np.eye(d)[j] for j in range(d)]\n",
        "\n",
        "            best_gcv = np.inf\n",
        "            best_tuple = None  # (a, knot, sign, col)\n",
        "\n",
        "            for a in candidate_dirs:\n",
        "                proj = self._project(X, a)\n",
        "                # candidate knots: percentyle w zakresie [10,90]\n",
        "                nknots = max(2, min(12, n // max(1, self.min_leaf)))\n",
        "                percentiles = np.linspace(10, 90, nknots)\n",
        "                knots = np.unique(np.percentile(proj, percentiles))\n",
        "                for knot in knots:\n",
        "                    for sign in (+1, -1):\n",
        "                        if sign == 1:\n",
        "                            col = np.maximum(proj - knot, 0.0)[:, None]\n",
        "                        else:\n",
        "                            col = np.maximum(knot - proj, 0.0)[:, None]\n",
        "                        if np.allclose(col, 0.0):\n",
        "                            continue\n",
        "                        D = np.hstack([design, col])\n",
        "                        coef_try, *_ = np.linalg.lstsq(D, y, rcond=None)\n",
        "                        y_try = D.dot(coef_try)\n",
        "                        gcv = self._gcv(y, y_try, D.shape[1])\n",
        "                        if gcv < best_gcv - 1e-12:\n",
        "                            best_gcv = gcv\n",
        "                            best_tuple = (a.copy(), float(knot), int(sign), col)\n",
        "\n",
        "            if best_tuple is None:\n",
        "                break\n",
        "            # accept basis\n",
        "            a_best, knot_best, sign_best, col_best = best_tuple\n",
        "            self.basis.append((a_best, knot_best, sign_best))\n",
        "            design = np.hstack([design, col_best])\n",
        "\n",
        "        # final fit\n",
        "        coef_final, *_ = np.linalg.lstsq(design, y, rcond=None)\n",
        "        self.intercept_ = float(coef_final[0])\n",
        "        if design.shape[1] > 1:\n",
        "            self.coefs = coef_final[1:].astype(float)\n",
        "        else:\n",
        "            self.coefs = np.array([], dtype=float)\n",
        "\n",
        "        # backward pruning based on GCV (try removing each basis if improves GCV)\n",
        "        improved = True\n",
        "        while improved and len(self.basis) > 0:\n",
        "            improved = False\n",
        "            # build design once per iteration\n",
        "            D_full = np.ones((n, 1))\n",
        "            for (a, knot, sign) in self.basis:\n",
        "                proj = self._project(X, a)\n",
        "                col = (np.maximum(proj - knot, 0.0) if sign == 1 else np.maximum(knot - proj, 0.0))[:, None]\n",
        "                D_full = np.hstack([D_full, col])\n",
        "            coef_full, *_ = np.linalg.lstsq(D_full, y, rcond=None)\n",
        "            gcv_full = self._gcv(y, D_full.dot(coef_full), D_full.shape[1])\n",
        "            # test removals\n",
        "            for idx in range(len(self.basis)):\n",
        "                basis_try = self.basis[:idx] + self.basis[idx + 1:]\n",
        "                D_try = np.ones((n, 1))\n",
        "                for (a, knot, sign) in basis_try:\n",
        "                    proj = self._project(X, a)\n",
        "                    col = (np.maximum(proj - knot, 0.0) if sign == 1 else np.maximum(knot - proj, 0.0))[:, None]\n",
        "                    D_try = np.hstack([D_try, col])\n",
        "                coef_try, *_ = np.linalg.lstsq(D_try, y, rcond=None)\n",
        "                gcv_try = self._gcv(y, D_try.dot(coef_try), D_try.shape[1])\n",
        "                if gcv_try < gcv_full - 1e-12:\n",
        "                    # accept pruning\n",
        "                    self.basis = basis_try\n",
        "                    self.intercept_ = float(coef_try[0])\n",
        "                    self.coefs = (coef_try[1:].astype(float) if D_try.shape[1] > 1 else np.array([], dtype=float))\n",
        "                    improved = True\n",
        "                    break\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n = X.shape[0]\n",
        "        y_pred = np.full(n, self.intercept_, dtype=float)\n",
        "        for idx, (a, knot, sign) in enumerate(self.basis):\n",
        "            proj = self._project(X, a)\n",
        "            term = (np.maximum(proj - knot, 0.0) if sign == 1 else np.maximum(knot - proj, 0.0))\n",
        "            coef = self.coefs[idx] if idx < len(self.coefs) else 0.0\n",
        "            y_pred += coef * term\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# LSM + faza II (L_hat, U_hat) używając ExtendedMARS\n",
        "# ---------------------------\n",
        "class LSM_MARS_QMC:\n",
        "    def __init__(self, simulator_func, payoff_func, times, r, dim, mars_params=None, rng_seed=None):\n",
        "        \"\"\"\n",
        "        simulator_func: callable(normals) -> paths shape (n_paths, n_steps, dim)\n",
        "          where normals shape is (n_paths, n_steps, dim) standard normals.\n",
        "        payoff_func: callable(t_index, x_vector) -> scalar payoff\n",
        "        times: array of time grid (length n_steps)\n",
        "        r: interest rate (annual)\n",
        "        dim: dimension of underlying (liczba aktywów)\n",
        "        mars_params: dict for ExtendedMARS\n",
        "        rng_seed: optional seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.simulator = simulator_func\n",
        "        self.payoff = payoff_func\n",
        "        self.times = np.asarray(times)\n",
        "        self.dt = np.diff(self.times)\n",
        "        self.r = float(r)\n",
        "        self.dim = int(dim)\n",
        "        self.mars_params = mars_params or {}\n",
        "        self.rng = np.random.default_rng(rng_seed)\n",
        "        # model parameters (mu, sigma) needed for simulator_one_step in phase II\n",
        "        self.mu = None\n",
        "        self.sigma = None\n",
        "\n",
        "    def set_model_params(self, mu, sigma):\n",
        "        self.mu = np.asarray(mu, dtype=float)\n",
        "        self.sigma = np.asarray(sigma, dtype=float)\n",
        "\n",
        "    def simulator_one_step(self, x_prev, z_stdnormal, step_index):\n",
        "        \"\"\"\n",
        "        Jednokrokowa symulacja GBM (wektorowa). Zakładamy model GBM dla każdego aktywa niezależnie.\n",
        "        x_prev: (dim,)\n",
        "        z_stdnormal: (dim,) standard normals\n",
        "        step_index: indeks kroku docelowego (j), symulujemy z t_{j-1} -> t_j\n",
        "        \"\"\"\n",
        "        if self.mu is None or self.sigma is None:\n",
        "            raise RuntimeError(\"set_model_params(mu, sigma) must be called before simulator_one_step.\")\n",
        "        # dt for step from t_{j-1} to t_j\n",
        "        if step_index <= 0:\n",
        "            raise ValueError(\"step_index must be >= 1 for one-step simulation\")\n",
        "        dt = self.times[step_index] - self.times[step_index - 1]\n",
        "        drift = (self.mu - 0.5 * (self.sigma ** 2)) * dt\n",
        "        diffusion = self.sigma * np.sqrt(dt) * z_stdnormal\n",
        "        return x_prev * np.exp(drift + diffusion)\n",
        "\n",
        "    def _generate_normals(self, n_paths, seed_offset=0):\n",
        "        n_steps = len(self.times)\n",
        "        total_dim = n_steps * self.dim\n",
        "        # seed_offset passed into seed for QMCPy for reproducibility; QMCPy expects integer seed or None\n",
        "        seed = None if seed_offset is None else int(seed_offset)\n",
        "        u = qmc_normals_qmcpy(n_paths, total_dim, seed=seed)\n",
        "        normals = u.reshape(n_paths, n_steps, self.dim)\n",
        "        return normals\n",
        "\n",
        "    def price(self, N1=1024, N2=512, max_terms=12, inner_MC=64):\n",
        "        \"\"\"\n",
        "        Wykonaj fazę I (N1) oraz fazę II (N2). Zwróć L_hat i U_hat.\n",
        "        inner_MC: liczba symulacji wewnętrznych do estymacji E[h_t | X_{t-1}]\n",
        "        \"\"\"\n",
        "        n_steps = len(self.times)\n",
        "        # Phase I: fit models on N1 QMC paths\n",
        "        normals1 = self._generate_normals(N1, seed_offset=11)\n",
        "        X1 = self.simulator(normals1)  # (N1, n_steps, dim)\n",
        "        # compute immediate payoffs matrix\n",
        "        B = np.zeros((N1, n_steps))\n",
        "        for i in range(N1):\n",
        "            for j in range(n_steps):\n",
        "                B[i, j] = float(self.payoff(j, X1[i, j, :]))\n",
        "\n",
        "        models = [None] * n_steps\n",
        "        # terminal\n",
        "        models[-1] = None  # terminal value is payoff\n",
        "\n",
        "        # backward induction LSM + MARS (fit B_j)\n",
        "        for j in range(n_steps - 2, -1, -1):\n",
        "            disc = np.exp(-self.r * self.dt[j])\n",
        "            # target: value = max(immediate payoff, discounted next)\n",
        "            Y = np.maximum(B[:, j], disc * B[:, j + 1])\n",
        "            X_train = X1[:, j, :]\n",
        "            model = ExtendedMARS(max_terms=max_terms, **self.mars_params)\n",
        "            model.fit(X_train, Y)\n",
        "            models[j] = model\n",
        "            cont = model.predict(X_train)\n",
        "            exercise = B[:, j] > cont\n",
        "            B[:, j] = np.where(exercise, B[:, j], disc * B[:, j + 1])\n",
        "\n",
        "        # Phase II: estimate L_hat and U_hat on independent N2 QMC paths\n",
        "        normals2 = self._generate_normals(N2, seed_offset=999)\n",
        "        X2 = self.simulator(normals2)\n",
        "        L_vals = np.zeros(N2)\n",
        "        U_vals = np.zeros(N2)\n",
        "\n",
        "        for i in range(N2):\n",
        "            pi = 0.0\n",
        "            stopped = False\n",
        "            for j in range(n_steps):\n",
        "                t = self.times[j]\n",
        "                g = float(self.payoff(j, X2[i, j, :]))\n",
        "                h = float(models[j].predict(X2[i, j, :].reshape(1, -1))[0]) if models[j] else g\n",
        "\n",
        "                if j > 0:\n",
        "                    x_prev = X2[i, j - 1, :]\n",
        "                    zs = self.rng.standard_normal(size=(inner_MC, self.dim))\n",
        "                    x_next_samples = np.array([self.simulator_one_step(x_prev, zs[k], j) for k in range(inner_MC)])\n",
        "                    h_vals = models[j].predict(x_next_samples) if models[j] else np.array([float(self.payoff(j, xn)) for xn in x_next_samples])\n",
        "                    h_cond = float(np.mean(h_vals))\n",
        "                else:\n",
        "                    h_cond = h  # zamiast 0\n",
        "\n",
        "                pi += h - h_cond  # bez dodatkowego dyskontowania\n",
        "\n",
        "                # Lower bound\n",
        "                if not stopped and g > h:\n",
        "                    L_vals[i] = np.exp(-self.r * t) * g\n",
        "                    stopped = True\n",
        "\n",
        "                # Upper bound\n",
        "                cand = np.exp(-self.r * t) * (g - pi)\n",
        "                if cand > U_vals[i]:\n",
        "                    U_vals[i] = cand\n",
        "\n",
        "            if not stopped:\n",
        "                L_vals[i] = np.exp(-self.r * self.times[-1]) * float(self.payoff(n_steps - 1, X2[i, -1, :]))\n",
        "\n",
        "        L_hat = float(np.mean(L_vals))\n",
        "        U_hat = float(np.mean(U_vals))\n",
        "        return {\"L_hat\": L_hat, \"U_hat\": U_hat, \"models\": models}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# GBM simulator (użyty jako przykład)\n",
        "# ---------------------------\n",
        "def gbm_simulator_factory(S0, mu, sigma, times):\n",
        "    \"\"\"\n",
        "    simulator(normals) -> paths with shape (n_paths, n_steps, dim)\n",
        "    normals expected shape: (n_paths, n_steps, dim) standard normals\n",
        "    \"\"\"\n",
        "    S0 = np.array(S0, dtype=float)\n",
        "    mu = np.array(mu, dtype=float)\n",
        "    sigma = np.array(sigma, dtype=float)\n",
        "    times = np.array(times, dtype=float)\n",
        "    dt = np.diff(times)\n",
        "    n_steps = len(times)\n",
        "    dim = len(S0)\n",
        "\n",
        "    def simulator(normals):\n",
        "        normals = np.asarray(normals)\n",
        "        n_paths = normals.shape[0]\n",
        "        paths = np.empty((n_paths, n_steps, dim), dtype=float)\n",
        "        for i in range(n_paths):\n",
        "            S = S0.copy()\n",
        "            paths[i, 0, :] = S\n",
        "            for j in range(1, n_steps):\n",
        "                z = normals[i, j, :]\n",
        "                S = S * np.exp((mu - 0.5 * sigma ** 2) * dt[j - 1] + sigma * np.sqrt(dt[j - 1]) * z)\n",
        "                paths[i, j, :] = S\n",
        "        return paths\n",
        "\n",
        "    return simulator\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Przykładowe uruchomienie\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # parametry modelu\n",
        "    S0 = [100.0]                # single asset\n",
        "    mu = np.array([0.03])\n",
        "    sigma = np.array([0.2])\n",
        "    r = 0.03\n",
        "    T = 1.0\n",
        "    Nsteps = 100\n",
        "    times = np.linspace(0.0, T, Nsteps + 1)\n",
        "\n",
        "    # payoff: american put on single asset\n",
        "    K = 100.0\n",
        "    payoff = lambda j, x: max(K - float(np.sum(x)), 0.0)\n",
        "\n",
        "    # stwórz symulator i pricer\n",
        "    sim = gbm_simulator_factory(S0, mu, sigma, times)\n",
        "    pricer = LSM_MARS_QMC(simulator_func=sim, payoff_func=payoff, times=times, r=r, dim=1,\n",
        "                          mars_params={\"min_leaf\": 5, \"penalty\": 3.0}, rng_seed=12345)\n",
        "    # ustaw parametry modelu potrzebne do simulator_one_step\n",
        "    pricer.set_model_params(mu=mu, sigma=sigma)\n",
        "\n",
        "    # uruchom (dobierz N1,N2,inner_MC w zależności od mocy obliczeniowej)\n",
        "    for N1 in (1024, 2048, 4096, 8192):\n",
        "        for N2 in (1024, 2048, 4096, 8192):\n",
        "            res = pricer.price(N1=N1, N2=N2, max_terms=8, inner_MC=32)\n",
        "            print(f\"Przedział wyceny: L̂ = {res['L_hat']:.6f}, Û = {res['U_hat']:.6f}, N1 = {N1}, N2 = {N2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Finalny skrypt: wycena opcji amerykańskiej (Ehrlichman-Henderson style)\n",
        "- QMC (qmcpy Sobol/Halton)\n",
        "- Extended MARS z LDA i GCV\n",
        "- LSM + martingale control variate\n",
        "- Poprawne dyskontowanie (używa dt)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from qmcpy import Sobol, Halton\n",
        "from typing import Callable, Tuple, Optional\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Pomocnicze funkcje\n",
        "# ---------------------------\n",
        "\n",
        "def _next_pow2(x: int) -> int:\n",
        "    if x <= 0:\n",
        "        return 1\n",
        "    return 1 << int(np.ceil(np.log2(x)))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Symulacja ścieżek QMC (z prefetch i dopełnieniem)\n",
        "# ---------------------------\n",
        "\n",
        "def simulate_paths(\n",
        "    S0: np.ndarray,\n",
        "    r: float,\n",
        "    sigma: np.ndarray,\n",
        "    rho: np.ndarray,\n",
        "    T: int,\n",
        "    N: int,\n",
        "    qmc_sampler,\n",
        "    seed: Optional[int] = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generuje N ścieżek d-wymiarowego Black-Scholesa z korelacjami.\n",
        "    Zwraca tablicę (N, T+1, d).\n",
        "    Uwaga: zakłada czas życia = 1 (rok). dt = 1/T.\n",
        "    \"\"\"\n",
        "    d = len(S0)\n",
        "    dt = 1.0 / T\n",
        "    paths = np.zeros((N, T + 1, d))\n",
        "    paths[:, 0, :] = S0\n",
        "\n",
        "    # Cholesky korelacji\n",
        "    L = np.linalg.cholesky(rho)\n",
        "\n",
        "    total_needed = N * T\n",
        "    n_to_request = _next_pow2(total_needed)\n",
        "    # Pobierz jednorazowo próbki QMC\n",
        "    u_all = qmc_sampler.gen_samples(n_min=0, n_max=n_to_request)  # (n_to_request, d)\n",
        "    u_all = u_all[:total_needed, :]\n",
        "\n",
        "    # Zapobiegamy dokładnym 0/1 (norm.ppf -> inf)\n",
        "    eps = 1e-12\n",
        "    u_all = np.clip(u_all, eps, 1 - eps)\n",
        "\n",
        "    # reshape to (T, N, d)\n",
        "    try:\n",
        "        u_by_t = u_all.reshape(T, N, d)\n",
        "    except Exception as e:\n",
        "        # awaryjnie: jeśli reshape nie działa (np. przy dziwnych wymiarach), użyj fallbacku\n",
        "        u_by_t = np.zeros((T, N, d))\n",
        "        for t in range(T):\n",
        "            start = t * N\n",
        "            u_by_t[t] = u_all[start:start + N]\n",
        "\n",
        "    for t in range(1, T + 1):\n",
        "        u = u_by_t[t - 1]  # (N, d)\n",
        "        z = norm.ppf(u)    # (N, d)\n",
        "        correlated_z = z @ L.T\n",
        "        drift = (r - 0.5 * sigma ** 2) * dt\n",
        "        diffusion = sigma * np.sqrt(dt) * correlated_z\n",
        "        paths[:, t, :] = paths[:, t - 1, :] * np.exp(drift + diffusion)\n",
        "\n",
        "    return paths\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Extended MARS z LDA + GCV\n",
        "# ---------------------------\n",
        "\n",
        "class ExtendedMARS:\n",
        "    \"\"\"\n",
        "    Extended MARS (ridge functions): kierunki = osie + LDA; selekcja liczby terminów przez GCV.\n",
        "    Model: f(x) = a0 + sum_j alpha_j * max(a_j^T x - k_j, 0)\n",
        "    \"\"\"\n",
        "    def __init__(self, max_terms: int = 20, knot_percentiles: Optional[list] = None):\n",
        "        self.max_terms = max_terms\n",
        "        self.knot_percentiles = knot_percentiles or [10,20,30,40,50,60,70,80,90]\n",
        "        self.a0 = 0.0\n",
        "        self.alphas = []\n",
        "        self.directions = []\n",
        "        self.knots = []\n",
        "\n",
        "    def _gcv(self, y, yhat, num_params):\n",
        "        n = len(y)\n",
        "        rss = np.sum((y - yhat)**2)\n",
        "        p = max(1, num_params) + 1  # effective params (at least intercept)\n",
        "        denom = (1 - p / n)**2\n",
        "        if denom <= 0:\n",
        "            return np.inf\n",
        "        return (rss / n) / denom\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        n, d = X.shape\n",
        "        self.a0 = float(np.mean(y))\n",
        "        residual = y - self.a0\n",
        "\n",
        "        # kandydackie kierunki: osie\n",
        "        directions = [np.eye(d)[i] for i in range(d)]\n",
        "\n",
        "        # dodaj kierunek LDA (o ile możliwe)\n",
        "        try:\n",
        "            # utworzymy binary label przez medianę (prosty split)\n",
        "            labels = (y > np.median(y)).astype(int)\n",
        "            if len(np.unique(labels)) > 1:\n",
        "                lda = LinearDiscriminantAnalysis(n_components=min(d, 1))\n",
        "                lda.fit(X, labels)\n",
        "                # scalings_ może mieć shape (d, n_components)\n",
        "                scal = np.atleast_2d(lda.scalings_).T\n",
        "                for vec in scal:\n",
        "                    v = vec.copy().astype(float)\n",
        "                    normv = np.linalg.norm(v)\n",
        "                    if normv > 0:\n",
        "                        v /= normv\n",
        "                        directions.append(v)\n",
        "        except Exception:\n",
        "            # jeśli LDA się nie uda — ignorujemy\n",
        "            pass\n",
        "\n",
        "        alphas = []\n",
        "        dirs = []\n",
        "        knots = []\n",
        "\n",
        "        preds = [np.full_like(y, self.a0)]\n",
        "        gcv_scores = [self._gcv(y, preds[0], 1)]\n",
        "\n",
        "        for term in range(1, self.max_terms + 1):\n",
        "            best_loss = np.sum(residual**2)\n",
        "            best_triplet = None\n",
        "            best_h = None\n",
        "\n",
        "            for a in directions:\n",
        "                proj = X @ a\n",
        "                candidates = np.percentile(proj, self.knot_percentiles)\n",
        "                for k in candidates:\n",
        "                    h = np.maximum(proj - k, 0.0)\n",
        "                    if np.var(h) < 1e-12:\n",
        "                        continue\n",
        "                    alpha = float(np.dot(residual, h) / (np.dot(h,h) + 1e-12))\n",
        "                    new_res = residual - alpha * h\n",
        "                    loss = np.sum(new_res**2)\n",
        "                    if loss < best_loss:\n",
        "                        best_loss = loss\n",
        "                        best_triplet = (alpha, a.copy(), float(k))\n",
        "                        best_h = h.copy()\n",
        "\n",
        "            if best_triplet is None:\n",
        "                break\n",
        "\n",
        "            alpha_star, a_star, k_star = best_triplet\n",
        "            alphas.append(alpha_star)\n",
        "            dirs.append(a_star)\n",
        "            knots.append(k_star)\n",
        "            residual -= alpha_star * best_h\n",
        "\n",
        "            # zbuduj predykcję przy bieżącym zestawie terminów\n",
        "            # unikamy sumowania pustej listy\n",
        "            comp = np.zeros_like(y)\n",
        "            for a_i, dir_i, k_i in zip(alphas, dirs, knots):\n",
        "                comp += a_i * np.maximum(X @ dir_i - k_i, 0.0)\n",
        "            pred = self.a0 + comp\n",
        "            preds.append(pred)\n",
        "            gcv_scores.append(self._gcv(y, pred, len(alphas)))\n",
        "\n",
        "            # wczesne zatrzymanie, jeśli GCV się pogorszy (mały heurystyczny warunek)\n",
        "            if len(gcv_scores) > 3 and gcv_scores[-1] > min(gcv_scores[:-1]):\n",
        "                break\n",
        "\n",
        "        # wybierz najlepszą liczbę terminów wg GCV\n",
        "        best_idx = int(np.argmin(gcv_scores))  # index w preds (0..len(preds)-1)\n",
        "        if best_idx == 0:\n",
        "            # tylko stała\n",
        "            self.alphas = []\n",
        "            self.directions = []\n",
        "            self.knots = []\n",
        "        else:\n",
        "            # zachowaj pierwsze best_idx terminów\n",
        "            self.alphas = alphas[:best_idx]\n",
        "            self.directions = dirs[:best_idx]\n",
        "            self.knots = knots[:best_idx]\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        n = X.shape[0]\n",
        "        out = np.full(n, self.a0)\n",
        "        for alpha, a, k in zip(self.alphas, self.directions, self.knots):\n",
        "            proj = X @ a\n",
        "            out += alpha * np.maximum(proj - k, 0.0)\n",
        "        return out\n",
        "\n",
        "    def conditional_expectation(self, X_prev: np.ndarray, r: float, sigma: np.ndarray, rho: np.ndarray, dt: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        E[h(X_t) | X_{t-1}=x_prev] dla termu h(x) = (a^T X_t - k)+ przy Black-Scholesie.\n",
        "        Zakładamy, że log S_t ~ N(log S_{t-1} + (r-0.5*sigma^2) dt, cov_matrix).\n",
        "        Dla h(x) = (a^T x - k)+ używamy, że a^T X_t jest kombinacją liniową wygenerowaną z lognormalnej zmiennej,\n",
        "        wykonujemy przybliżenie analogiczne do Black-Scholesa na projekcji.\n",
        "        \"\"\"\n",
        "        n, d = X_prev.shape\n",
        "        res = np.zeros(n)\n",
        "        cov = np.outer(sigma, sigma) * rho * dt\n",
        "        for i in range(n):\n",
        "            xprev = X_prev[i]\n",
        "            mu_vec = np.log(xprev) + (r - 0.5 * sigma**2) * dt\n",
        "            for alpha, a, k in zip(self.alphas, self.directions, self.knots):\n",
        "                mean_proj = float(a @ mu_vec)\n",
        "                var_proj = float(a @ cov @ a)\n",
        "                if k <= 0:\n",
        "                    # jeśli k <= 0 to (a^T X_t - k)+ = a^T X_t - k (zawsze dodatnie)\n",
        "                    if var_proj <= 0:\n",
        "                        exp_axt = np.exp(mean_proj)\n",
        "                    else:\n",
        "                        exp_axt = np.exp(mean_proj + 0.5 * var_proj)\n",
        "                    h_exp = exp_axt - k\n",
        "                else:\n",
        "                    if var_proj <= 1e-16:\n",
        "                        # niemal deterministyczne: przyjacielskie przybliżenie\n",
        "                        mean_val = np.exp(mean_proj)\n",
        "                        h_exp = max(mean_val - k, 0.0)\n",
        "                    else:\n",
        "                        std = np.sqrt(var_proj)\n",
        "                        # u = ln(k)\n",
        "                        u = np.log(k)\n",
        "                        d1 = (mean_proj - u + 0.5 * var_proj) / std\n",
        "                        d2 = d1 - std\n",
        "                        term1 = np.exp(mean_proj + 0.5 * var_proj) * norm.cdf(d1)\n",
        "                        term2 = k * norm.cdf(d2)\n",
        "                        h_exp = max(term1 - term2, 0.0)\n",
        "                res[i] += alpha * h_exp\n",
        "        return res + self.a0\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Główna funkcja wyceny\n",
        "# ---------------------------\n",
        "\n",
        "def price_american_option(\n",
        "    payoff_fn: Callable[[np.ndarray], float],\n",
        "    d: int,\n",
        "    n_steps: int,\n",
        "    r: float,\n",
        "    sigma: float,\n",
        "    S0: float,\n",
        "    K: float,\n",
        "    N1: int = 2048,\n",
        "    N2: int = 4096,\n",
        "    qmc_method: str = 'sobol_scrambled',\n",
        "    seed: Optional[int] = None,\n",
        "    max_mars_terms: int = 15,\n",
        "    rho_corr: Optional[np.ndarray] = None\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Zwraca (lower_bound, upper_bound, point_estimate)\n",
        "    Czas życia opcji = 1 rok; n_steps = liczba kroków (T).\n",
        "    \"\"\"\n",
        "    T = n_steps\n",
        "    dt = 1.0 / T\n",
        "\n",
        "    if rho_corr is None:\n",
        "        rho_corr = np.eye(d)\n",
        "    else:\n",
        "        assert rho_corr.shape == (d, d)\n",
        "\n",
        "    S0_vec = np.full(d, S0)\n",
        "    sigma_vec = np.full(d, sigma)\n",
        "\n",
        "    # inicjalizacja QMC samplera\n",
        "    if qmc_method == 'halton':\n",
        "        qmc1 = Halton(dimension=d, seed=seed)\n",
        "    elif qmc_method == 'sobol':\n",
        "        qmc1 = Sobol(dimension=d, seed=seed)\n",
        "    elif qmc_method == 'sobol_scrambled':\n",
        "        qmc1 = Sobol(dimension=d, randomize='Owen', seed=seed)\n",
        "    else:\n",
        "        raise ValueError(\"Nieznana metoda QMC\")\n",
        "\n",
        "    # Faza I: dopasowanie strategii i MARS\n",
        "    paths1 = simulate_paths(S0_vec, r, sigma_vec, rho_corr, T, N1, qmc1, seed)\n",
        "    Y = np.array([payoff_fn(paths1[n, T, :]) for n in range(N1)])\n",
        "    cv = np.zeros(N1)\n",
        "\n",
        "    mars_models = [None] * (T + 1)\n",
        "    # terminal model: tylko stała, odpowiada wartości Y\n",
        "    mars_models[T] = ExtendedMARS(max_terms=1)\n",
        "    mars_models[T].a0 = float(np.mean(Y))\n",
        "    mars_models[T].alphas = []\n",
        "    mars_models[T].directions = []\n",
        "    mars_models[T].knots = []\n",
        "\n",
        "    alpha_ls = [None] * T  # regresje LSM\n",
        "\n",
        "    for t in range(T - 1, -1, -1):\n",
        "        X_next = paths1[:, t + 1, :]\n",
        "        mars = ExtendedMARS(max_terms=max_mars_terms)\n",
        "        mars.fit(X_next, Y)\n",
        "        mars_models[t + 1] = mars\n",
        "\n",
        "        X_curr = paths1[:, t, :]\n",
        "        exp_mars = mars.conditional_expectation(X_curr, r, sigma_vec, rho_corr, dt)\n",
        "        cv += mars.predict(X_next) - exp_mars\n",
        "\n",
        "        # ITM mask\n",
        "        itm = np.array([payoff_fn(paths1[n, t, :]) > 0 for n in range(N1)])\n",
        "        if not np.any(itm):\n",
        "            alpha_ls[t] = np.zeros(1 + 2 * d)\n",
        "            # discount Y and cv for next step\n",
        "            Y *= np.exp(-r * dt)\n",
        "            cv *= np.exp(-r * dt)\n",
        "            continue\n",
        "\n",
        "        X_itm = X_curr[itm]\n",
        "        # basis: 1, x_i, x_i^2 (raw)\n",
        "        phi = np.hstack([np.ones((X_itm.shape[0], 1)), X_itm, X_itm ** 2])\n",
        "        Y_reg = Y[itm] - cv[itm]\n",
        "\n",
        "        # least squares — stabilność\n",
        "        try:\n",
        "            alpha_t, *_ = np.linalg.lstsq(phi, Y_reg, rcond=None)\n",
        "            alpha_t = alpha_t.ravel()\n",
        "        except np.linalg.LinAlgError:\n",
        "            alpha_t = np.zeros(phi.shape[1])\n",
        "\n",
        "        alpha_ls[t] = alpha_t\n",
        "\n",
        "        # aktualizacja decyzji (backward induction) — zastosuj dyskontowanie przy przejściu wstecz\n",
        "        for n in range(N1):\n",
        "            exercise = payoff_fn(paths1[n, t, :])\n",
        "            if itm[n]:\n",
        "                cont = float(alpha_t @ np.hstack([1.0, paths1[n, t, :], paths1[n, t, :] ** 2]))\n",
        "            else:\n",
        "                cont = -np.inf\n",
        "            if exercise > cont:\n",
        "                Y[n] = exercise\n",
        "                cv[n] = 0.0\n",
        "            else:\n",
        "                # discount to next time step\n",
        "                Y[n] *= np.exp(-r * dt)\n",
        "                cv[n] *= np.exp(-r * dt)\n",
        "\n",
        "    # Faza II: estymacja ograniczeń\n",
        "    # drugi sampler QMC (inna sekwencja przez zmieniony seed)\n",
        "    if qmc_method == 'halton':\n",
        "        qmc2 = Halton(dimension=d, seed=(seed + 1) if seed is not None else None)\n",
        "    elif qmc_method == 'sobol':\n",
        "        qmc2 = Sobol(dimension=d, seed=(seed + 1) if seed is not None else None)\n",
        "    elif qmc_method == 'sobol_scrambled':\n",
        "        qmc2 = Sobol(dimension=d, randomize='Owen', seed=(seed + 1) if seed is not None else None)\n",
        "    else:\n",
        "        raise ValueError(\"Nieznana metoda QMC\")\n",
        "\n",
        "    paths2 = simulate_paths(S0_vec, r, sigma_vec, rho_corr, T, N2, qmc2, seed)\n",
        "\n",
        "    # Martyngał kontrolny pi: akumulacja z dyskontowaniem zgodnie z formułą\n",
        "    # pi_t = pi_{t-1} + exp(-r*(t-1)*dt) * (h_t - E[h_t|F_{t-1}])\n",
        "    pi = np.zeros((N2, T + 1))\n",
        "    for t in range(1, T + 1):\n",
        "        X_t = paths2[:, t, :]\n",
        "        h_t = mars_models[t].predict(X_t)\n",
        "        X_prev = paths2[:, t - 1, :]\n",
        "        exp_h = mars_models[t].conditional_expectation(X_prev, r, sigma_vec, rho_corr, dt)\n",
        "        pi[:, t] = pi[:, t - 1] + np.exp(-r * (t - 1) * dt) * (h_t - exp_h)\n",
        "\n",
        "    # Strategia wykonania (LSM używając alpha_ls)\n",
        "    tau = np.full(N2, T, dtype=int)\n",
        "    for n in range(N2):\n",
        "        for t in range(T):\n",
        "            exercise = payoff_fn(paths2[n, t, :])\n",
        "            alpha_t = alpha_ls[t]\n",
        "            if alpha_t is None:\n",
        "                cont = -np.inf\n",
        "            else:\n",
        "                cont = float(alpha_t @ np.hstack([1.0, paths2[n, t, :], paths2[n, t, :] ** 2]))\n",
        "            if exercise > cont:\n",
        "                tau[n] = t\n",
        "                break\n",
        "\n",
        "    # zdyskontowane wypłaty wykonania\n",
        "    disc_payoff = np.exp(-r * tau * dt) * np.array([payoff_fn(paths2[n, tau[n], :]) for n in range(N2)])\n",
        "    lower = np.mean(disc_payoff - pi[np.arange(N2), tau])\n",
        "\n",
        "    # upper bound: sup_t [discounted payoff - pi_t]\n",
        "    upper_vals = []\n",
        "    for n in range(N2):\n",
        "        vals = [np.exp(-r * t * dt) * payoff_fn(paths2[n, t, :]) - pi[n, t] for t in range(T + 1)]\n",
        "        upper_vals.append(max(vals))\n",
        "    upper = np.mean(upper_vals)\n",
        "\n",
        "    # punktowa estymata: średnia LB/UB (możesz też użyć lower)\n",
        "    point = 0.5 * (lower + upper)\n",
        "\n",
        "    return lower, upper, point\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Przykład użycia (test)\n",
        "# ---------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # parametry testowe (1-roczna opcja, T kroków)\n",
        "    d = 1\n",
        "    T = 30         # liczba kroków (miesięczna dyskretyzacja)\n",
        "    r = 0.05\n",
        "    sigma = 0.2\n",
        "    S0 = 100.0\n",
        "    K = 100.0\n",
        "    rho = np.eye(d) * 1.0\n",
        "    payoff = lambda x: max(x[0] - K, 0.0)  # single-asset call (dla d=1)\n",
        "\n",
        "    lb, ub, pe = price_american_option(\n",
        "        payoff_fn=payoff,\n",
        "        d=d,\n",
        "        n_steps=T,\n",
        "        r=r,\n",
        "        sigma=sigma,\n",
        "        S0=S0,\n",
        "        K=K,\n",
        "        N1=1024,\n",
        "        N2=1024,\n",
        "        qmc_method='sobol_scrambled',\n",
        "        seed=42,\n",
        "        max_mars_terms=12,\n",
        "        rho_corr=rho\n",
        "    )\n",
        "\n",
        "    print(f\"Dolne ograniczenie: {lb:.4f}\")\n",
        "    print(f\"Górne ograniczenie: {ub:.4f}\")\n",
        "    print(f\"Szacowana cena:     {pe:.4f}\")\n",
        "    print(f\"Szerokość przedziału: {ub - lb:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZF-PsKPRpS4Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}